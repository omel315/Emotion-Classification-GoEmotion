{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1PIigLhBEjoU"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WBh4YUGAEmSr"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import classification_report, confusion_matrix, hamming_loss, roc_curve, auc\n","from sklearn.preprocessing import LabelBinarizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.preprocessing.text import Tokenizer\n","from keras.models import Sequential\n","from keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n","from keras.callbacks import EarlyStopping\n","import matplotlib.pyplot as plt\n","\n","# Load datasets\n","train_dataset_path = '/content/drive/MyDrive/Colab Notebooks/Thesis_Work/Thesis-II/DataFiles/full_dataset/train_dataset.csv'\n","validation_dataset_path = '/content/drive/MyDrive/Colab Notebooks/Thesis_Work/Thesis-II/DataFiles/full_dataset/validation_dataset.csv'\n","test_dataset_path = '/content/drive/MyDrive/Colab Notebooks/Thesis_Work/Thesis-II/DataFiles/full_dataset/test_dataset.csv'\n","\n","df_train = pd.read_csv(train_dataset_path)\n","validation_df = pd.read_csv(validation_dataset_path)\n","test_df = pd.read_csv(test_dataset_path)\n","\n","# Drop the 'emotion' column\n","df_train = df_train.drop(columns=['emotion'], errors='ignore')\n","validation_df = validation_df.drop(columns=['emotion'], errors='ignore')\n","test_df = test_df.drop(columns=['emotion'], errors='ignore')\n","\n","# Removing samples with only 0 in their labels\n","df_train = df_train.loc[ df_train.apply(lambda x: sum(x[1:]), axis=1)>0 ]\n","validation_df = validation_df.loc[ validation_df.apply(lambda x: sum(x[1:]), axis=1)>0 ]\n","test_df = test_df.loc[ test_df.apply(lambda x: sum(x[1:]), axis=1)>0 ]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":982},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1716371635731,"user":{"displayName":"8732 Omel","userId":"13043925165829484027"},"user_tz":-300},"id":"RdayH_pAt_zy","outputId":"7196224a-8a6f-454d-c365-e963e70b8a17"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df_train"},"text/html":["\n","  <div id=\"df-e1da3378-7213-4cf9-a6e9-b4d92e095f6c\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>cleaned_text</th>\n","      <th>admiration</th>\n","      <th>amusement</th>\n","      <th>anger</th>\n","      <th>annoyance</th>\n","      <th>approval</th>\n","      <th>caring</th>\n","      <th>confusion</th>\n","      <th>curiosity</th>\n","      <th>desire</th>\n","      <th>...</th>\n","      <th>love</th>\n","      <th>nervousness</th>\n","      <th>optimism</th>\n","      <th>pride</th>\n","      <th>realization</th>\n","      <th>relief</th>\n","      <th>remorse</th>\n","      <th>sadness</th>\n","      <th>surprise</th>\n","      <th>neutral</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>i was born in 98 so i feel like your 98 loss i...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>wow, you all are heroes!</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>if its not obvious everyone is having issues w...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>this architecture will be treasured even more ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>that sucks, i hope you find the finances to be...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>147842</th>\n","      <td>in what way is banning someone from using your...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>147843</th>\n","      <td>you would be floored to hear how many times i ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>147844</th>\n","      <td>what does obamacare have to do with that? name...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>147845</th>\n","      <td>that is not the typical sexual dynamic in west...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>147846</th>\n","      <td>my imaginary friend is always busy. i never ge...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>145436 rows Ã— 29 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1da3378-7213-4cf9-a6e9-b4d92e095f6c')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-e1da3378-7213-4cf9-a6e9-b4d92e095f6c button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e1da3378-7213-4cf9-a6e9-b4d92e095f6c');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-61c964c8-9ca3-484b-b6ec-2b21f65b4437\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-61c964c8-9ca3-484b-b6ec-2b21f65b4437')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-61c964c8-9ca3-484b-b6ec-2b21f65b4437 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_680c0966-da72-4869-b908-41a71214adf5\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df_train')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_680c0966-da72-4869-b908-41a71214adf5 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('df_train');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["                                             cleaned_text  admiration  \\\n","0       i was born in 98 so i feel like your 98 loss i...           0   \n","1                               wow, you all are heroes!            0   \n","2       if its not obvious everyone is having issues w...           0   \n","3       this architecture will be treasured even more ...           0   \n","4       that sucks, i hope you find the finances to be...           0   \n","...                                                   ...         ...   \n","147842  in what way is banning someone from using your...           0   \n","147843  you would be floored to hear how many times i ...           0   \n","147844  what does obamacare have to do with that? name...           0   \n","147845  that is not the typical sexual dynamic in west...           0   \n","147846  my imaginary friend is always busy. i never ge...           0   \n","\n","        amusement  anger  annoyance  approval  caring  confusion  curiosity  \\\n","0               0      0          0         1       0          0          0   \n","1               0      0          0         0       0          0          1   \n","2               0      0          0         0       0          0          0   \n","3               0      0          0         0       0          0          0   \n","4               0      0          0         0       0          0          0   \n","...           ...    ...        ...       ...     ...        ...        ...   \n","147842          0      0          0         0       0          1          0   \n","147843          0      0          0         0       0          0          0   \n","147844          0      0          0         0       0          0          0   \n","147845          0      0          1         0       0          0          0   \n","147846          0      0          0         0       0          0          0   \n","\n","        desire  ...  love  nervousness  optimism  pride  realization  relief  \\\n","0            0  ...     0            0         0      0            0       0   \n","1            0  ...     0            0         0      0            0       0   \n","2            0  ...     0            0         0      0            1       0   \n","3            0  ...     0            0         0      0            0       0   \n","4            0  ...     0            1         1      0            0       0   \n","...        ...  ...   ...          ...       ...    ...          ...     ...   \n","147842       0  ...     0            0         0      0            0       0   \n","147843       0  ...     0            0         0      0            0       0   \n","147844       0  ...     0            0         0      0            0       0   \n","147845       0  ...     0            0         0      0            0       0   \n","147846       0  ...     0            0         0      0            0       0   \n","\n","        remorse  sadness  surprise  neutral  \n","0             0        0         0        0  \n","1             0        0         1        0  \n","2             0        0         0        0  \n","3             0        0         0        1  \n","4             0        0         0        0  \n","...         ...      ...       ...      ...  \n","147842        0        0         0        0  \n","147843        0        0         0        1  \n","147844        0        0         0        1  \n","147845        0        0         0        0  \n","147846        0        1         0        0  \n","\n","[145436 rows x 29 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["df_train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lApcf_wDEuEX","outputId":"ac400a43-3ece-401f-e969-44dfcf28fb6e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","1137/1137 [==============================] - 1795s 2s/step - loss: 0.1640 - accuracy: 0.2779 - val_loss: 0.1429 - val_accuracy: 0.3393\n","Epoch 2/20\n","1137/1137 [==============================] - 1750s 2s/step - loss: 0.1399 - accuracy: 0.3609 - val_loss: 0.1331 - val_accuracy: 0.3719\n","Epoch 3/20\n","1137/1137 [==============================] - 1748s 2s/step - loss: 0.1330 - accuracy: 0.3789 - val_loss: 0.1286 - val_accuracy: 0.3861\n","Epoch 4/20\n"," 169/1137 [===>..........................] - ETA: 21:21 - loss: 0.1290 - accuracy: 0.3934"]}],"source":["\n","# Prepare datasets\n","X_train = df_train[\"cleaned_text\"].values\n","y_train = df_train.drop(columns=['cleaned_text']).values\n","X_val = validation_df[\"cleaned_text\"].values\n","y_val = validation_df.drop(columns=['cleaned_text']).values\n","X_test = test_df[\"cleaned_text\"].values\n","y_test = test_df.drop(columns=['cleaned_text']).values\n","\n","# Tokenization and Padding\n","max_words = 3000\n","max_len = 100\n","\n","tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n","tokenizer.fit_on_texts(X_train)\n","\n","X_train_seq = tokenizer.texts_to_sequences(X_train)\n","X_val_seq = tokenizer.texts_to_sequences(X_val)\n","X_test_seq = tokenizer.texts_to_sequences(X_test)\n","\n","X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n","X_val_pad = pad_sequences(X_val_seq, maxlen=max_len)\n","X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n","\n","# Build LSTM Model\n","model = Sequential()\n","model.add(Embedding(max_words, 128, input_length=max_len))\n","model.add(Bidirectional(LSTM(128, return_sequences=True)))\n","model.add(Dropout(0.5))\n","model.add(Bidirectional(LSTM(64)))\n","model.add(Dropout(0.5))\n","model.add(Dense(y_train.shape[1], activation='sigmoid'))\n","\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train Model\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","history = model.fit(X_train_pad, y_train, epochs=20, batch_size=128, validation_data=(X_val_pad, y_val), callbacks=[early_stopping])\n","\n","# Evaluate Model\n","y_train_pred = (model.predict(X_train_pad) > 0.5).astype(int)\n","y_val_pred = (model.predict(X_val_pad) > 0.5).astype(int)\n","y_test_pred = (model.predict(X_test_pad) > 0.5).astype(int)\n","\n","print(\"Training Classification Report:\\n\", classification_report(y_train, y_train_pred))\n","print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred))\n","print(\"Test Classification Report:\\n\", classification_report(y_test, y_test_pred))\n","\n","# Confusion Matrix and Hamming Loss\n","conf_matrix = confusion_matrix(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))\n","print(\"Confusion Matrix:\\n\", conf_matrix)\n","print(\"Hamming Loss:\", hamming_loss(y_test, y_test_pred))\n","\n","# ROC Curve\n","def plot_roc_curve(y_true, y_pred):\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","    for i in range(y_true.shape[1]):\n","        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","    plt.figure()\n","    for i in range(y_true.shape[1]):\n","        plt.plot(fpr[i], tpr[i], label=f'ROC curve of class {i} (area = {roc_auc[i]:.2f})')\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic')\n","    plt.legend(loc='lower right')\n","    plt.show()\n","\n","plot_roc_curve(y_test, y_test_pred)\n","\n","# Hyperparameter Tuning (Optional)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8eg1FzmeFALY"},"outputs":[],"source":["\n","# Prediction Function\n","def predict_samples(text_samples, model, tokenizer, max_len):\n","    text_samples_seq = tokenizer.texts_to_sequences(text_samples)\n","    text_samples_pad = pad_sequences(text_samples_seq, maxlen=max_len)\n","    samples_pred_labels = (model.predict(text_samples_pad) > 0.5).astype(int)\n","    samples_pred_labels_df = pd.DataFrame(samples_pred_labels)\n","    samples_pred_labels_df = samples_pred_labels_df.apply(lambda x: [emotions[i] for i in range(len(x)) if x[i]==1], axis=1)\n","    return pd.DataFrame({\"Text\": text_samples, \"Emotions\": list(samples_pred_labels_df)})\n","\n","# Predict new samples\n","test_text = df_train.sample(5)[\"cleaned_text\"].values.tolist()\n","print(predict_samples(test_text, model, tokenizer, max_len))\n"]},{"cell_type":"markdown","metadata":{"id":"iQuEJU8zLXTI"},"source":["# **Improved Version With Attention**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8R1JDKD0Ldgs"},"outputs":[],"source":["!pip install tensorflow\n","!pip install sklearn\n","!pip install matplotlib\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VKw5i6v_LkD5"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics import classification_report, confusion_matrix, hamming_loss, roc_curve, auc\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import to_categorical\n","import matplotlib.pyplot as plt\n","\n","# Load datasets\n","train_dataset_path = '/content/drive/MyDrive/Colab Notebooks/Thesis_Work/Thesis-II/DataFiles/full_dataset/train_dataset.csv'\n","validation_dataset_path = '/content/drive/MyDrive/Colab Notebooks/Thesis_Work/Thesis-II/DataFiles/full_dataset/validation_dataset.csv'\n","test_dataset_path = '/content/drive/MyDrive/Colab Notebooks/Thesis_Work/Thesis-II/DataFiles/full_dataset/test_dataset.csv'\n","\n","df_train = pd.read_csv(train_dataset_path)\n","validation_df = pd.read_csv(validation_dataset_path)\n","test_df = pd.read_csv(test_dataset_path)\n","\n","# Drop the 'emotion' column\n","df_train = df_train.drop(columns=['emotion'], errors='ignore')\n","validation_df = validation_df.drop(columns=['emotion'], errors='ignore')\n","test_df = test_df.drop(columns=['emotion'], errors='ignore')\n","\n","# Removing samples with only 0 in their labels\n","df_train = df_train.loc[ df_train.apply(lambda x: sum(x[1:]), axis=1)>0 ]\n","validation_df = validation_df.loc[ validation_df.apply(lambda x: sum(x[1:]), axis=1)>0 ]\n","test_df = test_df.loc[ test_df.apply(lambda x: sum(x[1:]), axis=1)>0 ]\n","\n","\n","# Prepare datasets\n","X_train = df_train[\"cleaned_text\"].values\n","y_train = df_train.drop(columns=['cleaned_text']).values\n","X_val = validation_df[\"cleaned_text\"].values\n","y_val = validation_df.drop(columns=['cleaned_text']).values\n","X_test = test_df[\"cleaned_text\"].values\n","y_test = test_df.drop(columns=['cleaned_text']).values\n","\n","# Tokenization and Padding\n","max_words = 3000\n","max_len = 100\n","\n","tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')\n","tokenizer.fit_on_texts(X_train)\n","\n","X_train_seq = tokenizer.texts_to_sequences(X_train)\n","X_val_seq = tokenizer.texts_to_sequences(X_val)\n","X_test_seq = tokenizer.texts_to_sequences(X_test)\n","\n","X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n","X_val_pad = pad_sequences(X_val_seq, maxlen=max_len)\n","X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n","\n","# TF-IDF Vectorization\n","vectorizer = TfidfVectorizer(max_features=max_words, max_df=0.85)\n","X_train_tfidf = vectorizer.fit_transform(X_train)\n","X_val_tfidf = vectorizer.transform(X_val)\n","X_test_tfidf = vectorizer.transform(X_test)\n","\n","# Function to get top 10 important words\n","def get_top_tfidf_words(tfidf_vectorizer, response, top_n=10):\n","    sorted_nzs = np.argsort(response.data)[:-(top_n+1):-1]\n","    feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n","    return feature_names[sorted_nzs]\n","\n","# Get top 10 important words for each document\n","top_words_train = [get_top_tfidf_words(vectorizer, X_train_tfidf[i]) for i in range(X_train_tfidf.shape[0])]\n","top_words_val = [get_top_tfidf_words(vectorizer, X_val_tfidf[i]) for i in range(X_val_tfidf.shape[0])]\n","top_words_test = [get_top_tfidf_words(vectorizer, X_test_tfidf[i]) for i in range(X_test_tfidf.shape[0])]\n","\n","# Create attention mask based on top words\n","def create_attention_mask(tokenizer, sequences, top_words):\n","    attention_masks = []\n","    for seq, words in zip(sequences, top_words):\n","        mask = np.isin(seq, [tokenizer.word_index[word] for word in words if word in tokenizer.word_index])\n","        attention_masks.append(mask)\n","    return pad_sequences(attention_masks, maxlen=max_len, padding='post', truncating='post', value=0)\n","\n","train_attention_masks = create_attention_mask(tokenizer, X_train_seq, top_words_train)\n","val_attention_masks = create_attention_mask(tokenizer, X_val_seq, top_words_val)\n","test_attention_masks = create_attention_mask(tokenizer, X_test_seq, top_words_test)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMtbhfbfLn2b"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Bidirectional, Layer, Multiply\n","from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Attention Layer using TF-IDF scores\n","class AttentionLayer(Layer):\n","    def __init__(self, **kwargs):\n","        super(AttentionLayer, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], input_shape[-1]), initializer=\"glorot_uniform\", trainable=True)\n","        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[-1],), initializer=\"zeros\", trainable=True)\n","        self.u = self.add_weight(name=\"att_u\", shape=(input_shape[-1], 1), initializer=\"glorot_uniform\", trainable=True)\n","        super(AttentionLayer, self).build(input_shape)\n","\n","    def call(self, x, mask):\n","        # Compute attention scores\n","        u_t = tf.tanh(tf.tensordot(x, self.W, axes=1) + self.b)\n","        a_t = tf.tensordot(u_t, self.u, axes=1)\n","        a_t = tf.nn.softmax(a_t, axis=1)\n","        # Apply the mask\n","        a_t = a_t * mask\n","        a_t = a_t / tf.reduce_sum(a_t, axis=1, keepdims=True)\n","        # Compute context vector\n","        output = x * a_t\n","        return tf.reduce_sum(output, axis=1)\n","\n","# Build LSTM Model with Attention\n","input_layer = Input(shape=(max_len,))\n","mask_input = Input(shape=(max_len,))\n","embedding_layer = Embedding(input_dim=max_words, output_dim=128, input_length=max_len)(input_layer)\n","lstm_layer = Bidirectional(LSTM(128, return_sequences=True))(embedding_layer)\n","dropout_layer = Dropout(0.5)(lstm_layer)\n","attention_layer = AttentionLayer()([dropout_layer, mask_input])\n","dense_layer = Dense(64, activation='relu')(attention_layer)\n","dropout_layer_2 = Dropout(0.5)(dense_layer)\n","output_layer = Dense(y_train.shape[1], activation='sigmoid')(dropout_layer_2)\n","\n","model = Model(inputs=[input_layer, mask_input], outputs=output_layer)\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train Model\n","early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n","history = model.fit([X_train_pad, train_attention_masks], y_train, epochs=20, batch_size=128, validation_data=([X_val_pad, val_attention_masks], y_val), callbacks=[early_stopping])\n","\n","# Evaluate Model\n","y_train_pred = (model.predict([X_train_pad, train_attention_masks]) > 0.5).astype(int)\n","y_val_pred = (model.predict([X_val_pad, val_attention_masks]) > 0.5).astype(int)\n","y_test_pred = (model.predict([X_test_pad, test_attention_masks]) > 0.5).astype(int)\n","\n","print(\"Training Classification Report:\\n\", classification_report(y_train, y_train_pred))\n","print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred))\n","print(\"Test Classification Report:\\n\", classification_report(y_test, y_test_pred))\n","\n","# Confusion Matrix and Hamming Loss\n","conf_matrix = confusion_matrix(y_test.argmax(axis=1), y_test_pred.argmax(axis=1))\n","print(\"Confusion Matrix:\\n\", conf_matrix)\n","print(\"Hamming Loss:\", hamming_loss(y_test, y_test_pred))\n","\n","# ROC Curve\n","def plot_roc_curve(y_true, y_pred):\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","    for i in range(y_true.shape[1]):\n","        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_pred[:, i])\n","        roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","    plt.figure()\n","    for i in range(y_true.shape[1]):\n","        plt.plot(fpr[i], tpr[i], label=f'ROC curve of class {i} (area = {roc_auc[i]:.2f})')\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic')\n","    plt.legend(loc='lower right')\n","    plt.show()\n","\n","plot_roc_curve(y_test, y_test_pred)\n","\n","# Prediction Function\n","def predict_samples(text_samples, model, tokenizer, max_len):\n","    text_samples_seq = tokenizer.texts_to_sequences(text_samples)\n","    text_samples_pad = pad_sequences(text_samples_seq, maxlen=max_len)\n","    text_samples_tfidf = vectorizer.transform(text_samples)\n","    top_words_samples = [get_top_tfidf_words(vectorizer, text_samples_tfidf[i]) for i in range(text_samples_tfidf.shape[0])]\n","    attention_masks_samples = create_attention_mask(tokenizer, text_samples_seq, top_words_samples)\n","    samples_pred_labels = (model.predict([text_samples_pad, attention_masks_samples]) > 0.5).astype(int)\n","    samples_pred_labels_df = pd.DataFrame(samples_pred_labels)\n","    samples_pred_labels_df = samples_pred_labels_df.apply(lambda x: [emotions[i] for i in range(len(x)) if x[i]==1], axis=1)\n","    return pd.DataFrame({\"Text\": text_samples, \"Emotions\": list(samples_pred_labels_df)})\n","\n","# Predict new samples\n","test_text = df_train.sample(5)[\"cleaned_text\"].values.tolist()\n","print(predict_samples(test_text, model, tokenizer, max_len))\n"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNWDSsMMZpqIJUaRzsIk2TM"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}